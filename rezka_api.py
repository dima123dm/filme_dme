import os
import re
import json
from curl_cffi import requests as curl_requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

class RezkaClient:
    def __init__(self):
        # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –ø–æ–¥ Chrome
        self.session = curl_requests.Session(impersonate="chrome110")
        self.login = os.getenv("REZKA_LOGIN")
        self.password = os.getenv("REZKA_PASS")
        self.is_logged_in = False
        self.origin = "https://hdrezka.me"

    def auth(self):
        if self.is_logged_in: return True
        try:
            print("üîë –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...")
            headers = {"X-Requested-With": "XMLHttpRequest"}
            r = self.session.post(f"{self.origin}/ajax/login/", 
                                data={"login_name": self.login, "login_password": self.password},
                                headers=headers)
            if r.json().get('success'):
                self.is_logged_in = True
                print("‚úÖ –í—Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω")
                return True
        except: pass
        print("‚ùå –û—à–∏–±–∫–∞ –≤—Ö–æ–¥–∞")
        return False

    def _is_watched(self, tag):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∞–ª–æ—á–∫–∏ (–≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã)"""
        # 1. –ö–ª–∞—Å—Å –Ω–∞ —Å–∞–º–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
        classes = tag.get("class", [])
        if "watched" in classes or "b-watched" in classes:
            return True
        
        # 2. –ö–ª–∞—Å—Å –≤–Ω—É—Ç—Ä–∏ (–Ω–∞ –∏–∫–æ–Ω–∫–µ)
        # <i class="watch-episode-action watched">
        icon = tag.find(class_=lambda x: x and ("watch-episode-action" in x or "b-ico" in x))
        if icon:
            icon_classes = icon.get("class", [])
            if "watched" in icon_classes or "b-watched" in icon_classes:
                return True
                
        return False

    def _parse_html_list(self, html_content):
        """–†–∞–∑–±–∏—Ä–∞–µ—Ç HTML, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏—Å–ª–∞–ª API"""
        soup = BeautifulSoup(html_content, 'html.parser')
        seasons = {}
        
        # –ò—â–µ–º –≤—Å–µ li (—Å–µ—Ä–∏–∏)
        items = soup.find_all("li", class_="b-simple_episode__item")
        
        for item in items:
            try:
                s_id = item.get("data-season_id", "1")
                e_id = item.get("data-episode_id", "1")
                title = item.text.strip()
                
                # ID –¥–ª—è –≥–∞–ª–æ—á–∫–∏
                global_id = item.get("data-id")
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∞ li, –∏—â–µ–º –≤–Ω—É—Ç—Ä–∏
                if not global_id:
                    inner = item.find(attrs={"data-id": True})
                    if inner: global_id = inner.get("data-id")

                # –°—Ç–∞—Ç—É—Å
                is_watched = self._is_watched(item)

                if s_id not in seasons: seasons[s_id] = []
                seasons[s_id].append({
                    "title": title, "episode": e_id, 
                    "global_id": global_id, "watched": is_watched
                })
            except: continue
            
        return seasons

    def get_series_details(self, url):
        if not self.auth(): return {"error": "Auth failed"}
        try:
            print(f"üîé –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
            r = self.session.get(url)
            soup = BeautifulSoup(r.text, 'html.parser')
            
            # 1. –ü–æ—Å—Ç–µ—Ä HD
            hq_poster = ""
            side = soup.find(class_="b-sidecover")
            if side:
                if side.find('a'): hq_poster = side.find('a').get('href')
                elif side.find('img'): hq_poster = side.find('img').get('src')

            # 2. –ò—â–µ–º ID –ø–æ—Å—Ç–∞
            post_id = None
            if soup.find(id="post_id"): 
                post_id = soup.find(id="post_id").get("value")
            else:
                match = re.search(r'["\']post_id["\']\s*:\s*(\d+)', r.text)
                if match: post_id = match.group(1)

            if not post_id:
                return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ ID —Å–µ—Ä–∏–∞–ª–∞", "poster": hq_poster}

            # 3. –ò—â–µ–º ID –û–∑–≤—É—á–∫–∏ (Translator ID)
            translator_id = None
            
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –∞–∫—Ç–∏–≤–Ω—É—é (—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–±—Ä–∞–Ω–∞ —É —Ç–µ–±—è –≤ –ø—Ä–æ—Ñ–∏–ª–µ –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            active_trans = soup.find(class_="b-translator__item active")
            if active_trans:
                translator_id = active_trans.get("data-translator_id")
                print(f"üéô –ù–∞—à–µ–ª –∞–∫—Ç–∏–≤–Ω—É—é –æ–∑–≤—É—á–∫—É ID: {translator_id}")
            
            # –ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ–π –Ω–µ—Ç (–±—ã–≤–∞–µ—Ç, –µ—Å–ª–∏ –æ–∑–≤—É—á–∫–∞ –≤—Å–µ–≥–æ –æ–¥–Ω–∞), –∏—â–µ–º –≤ —Å–∫—Ä–∏–ø—Ç–∞—Ö
            if not translator_id:
                match = re.search(r'["\']translator_id["\']\s*:\s*(\d+)', r.text)
                if match: 
                    translator_id = match.group(1)
                    print(f"üéô –ù–∞—à–µ–ª —Å–∫—Ä—ã—Ç—É—é –æ–∑–≤—É—á–∫—É ID: {translator_id}")

            # 4. –î–ï–õ–ê–ï–ú –ó–ê–ü–†–û–° –ö API –ó–ê –í–°–ï–ú–ò –°–ï–ó–û–ù–ê–ú–ò
            # –î–∞–∂–µ –µ—Å–ª–∏ translator_id –Ω–µ—Ç (null), API –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –æ–∑–≤—É—á–∫—É
            print(f"üöÄ –ó–∞–ø—Ä–∞—à–∏–≤–∞—é –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π —á–µ—Ä–µ–∑ API (ID: {post_id})...")
            
            payload = {
                "id": post_id,
                "translator_id": translator_id if translator_id else "238", # 238 —á–∞—Å—Ç–æ default
                "action": "get_episodes"
            }
            
            r_ajax = self.session.post(f"{self.origin}/ajax/get_cdn_series/", data=payload)
            data = r_ajax.json()
            
            if data.get('success'):
                # API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML —Å–æ –í–°–ï–ú–ò —Å–µ–∑–æ–Ω–∞–º–∏
                html = data.get('seasons') or data.get('episodes')
                seasons = self._parse_html_list(html)
                
                if seasons:
                    print(f"‚úÖ –£—Å–ø–µ—Ö! –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–µ–∑–æ–Ω–æ–≤: {len(seasons)}")
                    return {"seasons": seasons, "poster": hq_poster, "post_id": post_id}
            
            # –ï—Å–ª–∏ API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç—Ç–æ —Ñ–∏–ª—å–º), –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å —Å–∞–º—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print("‚ö†Ô∏è API –Ω–µ –≤–µ—Ä–Ω—É–ª —Å–µ—Ä–∏–π. –ü—Ä–æ–±—É—é –ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É (–≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Ñ–∏–ª—å–º)...")
            seasons = self._parse_html_list(r.text)
            if seasons:
                 return {"seasons": seasons, "poster": hq_poster, "post_id": post_id}

            return {"error": "–°–µ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã", "poster": hq_poster, "post_id": post_id}

        except Exception as e:
            print(f"CRITICAL ERROR: {e}")
            return {"error": str(e)}

    # --- –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---
    def get_category_items(self, cat_id):
        if not self.auth(): return []
        try:
            r = self.session.get(f"{self.origin}/favorites/{cat_id}/")
            soup = BeautifulSoup(r.text, 'html.parser')
            items = []
            for item in soup.find_all(class_="b-content__inline_item"):
                try:
                    link = item.find(class_="b-content__inline_item-link").find("a")
                    img = item.find(class_="b-content__inline_item-cover").find("img")
                    status = item.find(class_="info")
                    items.append({
                        "id": item.get("data-id"),
                        "title": link.text.strip(),
                        "url": link.get("href"),
                        "poster": img.get("src") if img else "",
                        "status": status.text.strip() if status else ""
                    })
                except: continue
            return items
        except: return []

    def search(self, query):
        if not self.auth(): return []
        try:
            r = self.session.post(f"{self.origin}/engine/ajax/search.php", data={"q": query})
            soup = BeautifulSoup(r.content, 'html.parser')
            results = []
            for item in soup.select('.b-search__section_list li'):
                try:
                    link = item.find('a')
                    title = item.find('span', class_='enty').get_text().strip()
                    url = link.attrs['href']
                    match = re.search(r'/(\d+)-', url)
                    if match:
                        results.append({
                            "id": match.group(1),
                            "title": title, "url": url
                        })
                except: continue
            return results
        except: return []

    def add_favorite(self, post_id, cat_id):
        if not self.auth(): return False
        try:
            r = self.session.post(f"{self.origin}/ajax/favorites/", data={
                "post_id": post_id, "cat_id": cat_id, "action": "add_post"
            })
            return r.json().get('success', False)
        except: return False

    def toggle_watch(self, global_id):
        if not self.auth(): return False
        try:
            r = self.session.post(f"{self.origin}/engine/ajax/schedule_watched.php", data={"id": global_id})
            return r.status_code == 200
        except: return False

client = RezkaClient()